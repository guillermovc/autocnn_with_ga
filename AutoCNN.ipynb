{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuCigbl9I9Tf"
      },
      "source": [
        "# Generating CNN architectures automatically with genetic algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7H7nvr-Jhfm",
        "outputId": "d63f94d7-34a5-4046-eba4-950bbca1a4ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import json\n",
        "import time\n",
        "import pprint\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tabulate import tabulate\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Add, Dense, Activation, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOxcyxXQzcPw"
      },
      "source": [
        "# ResNets\n",
        "\n",
        "This project is based in ResNets, using this kind of Convolutional Neural Networks allows us to make very deep neural networks avoiding gradient vanishing and overfitting issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "L9zdfg8s4UH3"
      },
      "outputs": [],
      "source": [
        "def skip_layer_encode():\n",
        "    f1 = 2 ** random.randint(5, 9) # number from 32 to 512\n",
        "    f2 = 2 ** random.randint(5, 9) # number from 32 to 512\n",
        "    return f\"{f1}_{f2}\"\n",
        "\n",
        "def pooling_layer_encode():\n",
        "    q = random.random()\n",
        "    if q < 0.5:\n",
        "        # Max Pooling\n",
        "        return \"max\"\n",
        "    else:\n",
        "        return \"mean\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "U8CkJclAuyqu"
      },
      "outputs": [],
      "source": [
        "def random_layer():\n",
        "    r = random.random()\n",
        "\n",
        "    if r < 0.5:\n",
        "        return skip_layer_encode()\n",
        "    else:\n",
        "        return pooling_layer_encode()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5b8ZOEHJkD_"
      },
      "source": [
        "## Skip Layer\n",
        "\n",
        "### What is a Skip Layer?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Oc_owT3Y5asP"
      },
      "outputs": [],
      "source": [
        "def skip_layer(X, f1, f2, kernel = (3,3), stride = (1,1)):\n",
        "    inputs = X\n",
        "\n",
        "    # First convolution\n",
        "    layer = Conv2D(f1, kernel_size=kernel, strides=stride, padding=\"same\")(X)\n",
        "    layer = BatchNormalization(axis=3)(layer)\n",
        "    layer = Activation(\"relu\")(layer)\n",
        "\n",
        "    # Second convolution\n",
        "    layer = Conv2D(f2, kernel_size=kernel, strides=stride, padding=\"same\")(layer)\n",
        "    layer = BatchNormalization(axis=3)(layer)\n",
        "\n",
        "    # Inter convolution (makes sure that the dimensionality at the skip layers are the same)\n",
        "    inputs = Conv2D(f2, kernel_size=(1,1), strides=stride, padding=\"same\")(inputs)\n",
        "\n",
        "    # We add the input and the second convolution layers\n",
        "    outputs = Add()([inputs, layer])\n",
        "    outputs = Activation(\"relu\")(outputs)\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opgXXCAZJn1e"
      },
      "source": [
        "## Pooling Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "XGnPjnGuQFYk"
      },
      "outputs": [],
      "source": [
        "def pooling_layer(X, pooling_type, kernel = (2,2), stride = (2,2)):\n",
        "    pooling_choices = {\n",
        "        \"max\": MaxPooling2D,\n",
        "        \"mean\": AveragePooling2D\n",
        "    }\n",
        "\n",
        "    return pooling_choices[pooling_type](pool_size=kernel, strides=stride, padding=\"same\")(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_BU9tZpqJnM"
      },
      "source": [
        "# Genetic Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXIEZ5hTqLm-"
      },
      "source": [
        "## The CNN class (individual)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "K2yD0lBe5Emr"
      },
      "outputs": [],
      "source": [
        "# This class represents each individual of our population\n",
        "class CNN:\n",
        "\n",
        "    def __init__(self, encoding:str, input_shape:tuple, output_shape:int) -> None:\n",
        "\n",
        "        \"\"\"\n",
        "        Class constructor\n",
        "\n",
        "        Args:\n",
        "            encoding (str): Encoding representation of the CNN\n",
        "            input_shape (tuple): Input shape of the CNN (height, width, channels)\n",
        "            output_shape (int): Number of classes of the CNN\n",
        "        \"\"\"\n",
        "\n",
        "        # Genetic algorithm stuff\n",
        "        self.genes = encoding.split(\"-\") # List of genes (cnn layers)\n",
        "        self.num_genes = len(self.genes)\n",
        "        self.fitness = 0.0               # Adaptation value\n",
        "\n",
        "        # Convolutional Neural Network stuff\n",
        "        self.encoding = encoding         # Encoded representation of the CNN\n",
        "        self.input_shape = input_shape   # Input shape (WIDTH, HEIGHT, CHANNELS)\n",
        "        self.output_shape = output_shape # Output shape (number of classes)\n",
        "        self.accuracy = 0.0              # Accuracy of the model\n",
        "        self.process_time = 0.0          # Time of the model to make an inference\n",
        "        self.training_time = 0.0         # Time taken to train\n",
        "\n",
        "    def generate_model(self) -> tf.keras.Model:\n",
        "        \"\"\"\n",
        "        Generates a Keras model from the encoding\n",
        "\n",
        "        Returns:\n",
        "            model: tensorflow.keras.Model\n",
        "        \"\"\"\n",
        "        inputs = Input(shape=self.input_shape)\n",
        "        outputs = inputs\n",
        "\n",
        "        # Create a list of layers from the encoding of the cnn\n",
        "        layers = []\n",
        "        for layer in self.encoding.split(\"-\"):\n",
        "            if layer == \"mean\":\n",
        "                outputs = pooling_layer(outputs, \"mean\")\n",
        "            elif layer == \"max\":\n",
        "                outputs=  pooling_layer(outputs, \"max\")\n",
        "            else:\n",
        "                # Skip layer\n",
        "                f1, f2 = layer.split(\"_\")\n",
        "                outputs = skip_layer(outputs, int(f1), int(f2))\n",
        "\n",
        "        outputs = GlobalMaxPooling2D()(outputs)\n",
        "        outputs = Dense(128, activation=\"relu\")(outputs)\n",
        "        outputs = Dense(self.output_shape, activation=\"softmax\")(outputs)\n",
        "\n",
        "        return Model(inputs = inputs, outputs = outputs)\n",
        "\n",
        "    def compute_fitness(self):\n",
        "        self.fitness = self.accuracy / self.process_time\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "\n",
        "        \"\"\"\n",
        "        String representation of the object.\n",
        "        \"\"\"\n",
        "\n",
        "        return f\"\"\"Model encoding: {self.encoding}, \\nModel Accuracy: {self.accuracy},\\nModel process time: {self.process_time}\"\"\"\n",
        "\n",
        "    def get_info(self) -> dict:\n",
        "        \n",
        "        \"\"\"\n",
        "        Returns a dict with the information of the current individual.\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary containing class important attributes.\n",
        "        \"\"\"\n",
        "        \n",
        "        return {\n",
        "            \"encoding\" : self.encoding,\n",
        "            \"depth\": self.num_genes,\n",
        "            \"fitness\" : self.fitness,\n",
        "            \"accuracy\" : self.accuracy,\n",
        "            \"process time\": self.process_time,\n",
        "            \"training time\": self.training_time,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7ZatuHUqQdn"
      },
      "source": [
        "## The Population class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "qMSQ3sYLt5pY"
      },
      "outputs": [],
      "source": [
        "class Population:\n",
        "\n",
        "    def __init__(self, n_individuals, min_genes, max_genes):\n",
        "        self.n_individuals = n_individuals\n",
        "        self.min_genes = min_genes\n",
        "        self.max_genes = max_genes\n",
        "        self.individuals = None\n",
        "        self.best_individual = None\n",
        "        self.mean_adaptation = 0.0\n",
        "\n",
        "    def initialize(self, input_shape, output_shape):\n",
        "        population = [] # List of CNN objects\n",
        "        \n",
        "        for _ in range(self.n_individuals):\n",
        "            new_individual = self.generate_individual(input_shape, output_shape)\n",
        "            population.append(new_individual)\n",
        "\n",
        "        self.individuals = population\n",
        "\n",
        "    def generate_individual(self, input_shape, output_shape):\n",
        "        \"\"\"_summary_\n",
        "\n",
        "        Args:\n",
        "            input_shape (tuple): Shape of the input (width, height, n_channels)\n",
        "            output_shape (int): Number of output classes\n",
        "\n",
        "        Returns:\n",
        "            CNN: Randomly initialized instance of the CNN class\n",
        "        \"\"\"\n",
        "        depth = random.randint(self.min_genes, self.max_genes)\n",
        "        layers = [random_layer() for _ in range(depth)]\n",
        "        layers = \"-\".join(layers)\n",
        "\n",
        "        return CNN(layers, input_shape, output_shape)\n",
        "\n",
        "    def compute_average_fitness(self):\n",
        "        self.mean_adaptation = sum(individual.accuracy for individual in self.individuals)\n",
        "\n",
        "    def print(self):\n",
        "        individuals_info = []\n",
        "        for individual in self.individuals:\n",
        "            individuals_info.append(\n",
        "                [individual.num_genes, individual.encoding, round(individual.accuracy, 4),\n",
        "                 round(individual.process_time, 4), round(individual.fitness, 4)]\n",
        "            )\n",
        "\n",
        "        print(tabulate(individuals_info,\n",
        "        headers=[\"CNN Depth\", \"Encoding\", \"Accuracy\", \"Process time\", \"Fitness\"],\n",
        "        numalign=\"center\", stralign=\"left\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CnH-TrUqSg_"
      },
      "source": [
        "## The GeneticAlgorithm class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "lyiZk9R-5VfC"
      },
      "outputs": [],
      "source": [
        "class GeneticAlgorithm:\n",
        "\n",
        "    def __init__(self, population_size, min_genes, max_genes, fitness_func, mutation_rate, \n",
        "                crossover_rate, num_generations, saved_cnns, training_params):\n",
        "        self.population_size = population_size\n",
        "        self.min_genes = min_genes\n",
        "        self.max_genes = max_genes\n",
        "        self.fitness_func = fitness_func\n",
        "        self.num_generations = num_generations\n",
        "        \n",
        "        self.mutation_rate = mutation_rate\n",
        "        self.crossover_rate = crossover_rate\n",
        "\n",
        "        self.population = Population(population_size, min_genes, max_genes)\n",
        "        self.input_shape = training_params[\"X_train\"][0].shape\n",
        "        self.output_shape = training_params[\"y_train_cat\"][0].shape[0]\n",
        "        \n",
        "        self.saved_cnns = saved_cnns\n",
        "        self.training_params = training_params\n",
        "\n",
        "    def save_architectures(self):\n",
        "        # Directly from dictionary\n",
        "        with open('evaluated_architectures.json', 'w') as outfile:\n",
        "            json.dump(self.saved_cnns, outfile, indent=4)\n",
        "\n",
        "    def evaluate_population(self):\n",
        "\n",
        "        # Get training params\n",
        "        X_train = self.training_params[\"X_train\"]\n",
        "        X_test = self.training_params[\"X_test\"]\n",
        "        y_train_cat = self.training_params[\"y_train_cat\"]\n",
        "        y_test_cat = self.training_params[\"y_test_cat\"]\n",
        "        epochs = self.training_params[\"epochs\"]\n",
        "        batch_size = self.training_params[\"batch_size\"]\n",
        "\n",
        "        if self.population.best_individual is not None:\n",
        "            best_acc = self.population.best_individual.accuracy\n",
        "        else:\n",
        "            best_acc = 0.0\n",
        "\n",
        "        for individual in self.population.individuals:\n",
        "            print(\"\".center(100, \"=\"))\n",
        "            # Check if our architecture is already in the saved cnns dict\n",
        "            if individual.encoding in self.saved_cnns:\n",
        "                print(f\"Architecture {individual.encoding} already evaluated\")\n",
        "            else:\n",
        "                model = individual.generate_model()\n",
        "                model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "                print(f\"Architecture: {individual.encoding}, Depth: {individual.num_genes}\")\n",
        "                print(\"Training ...\")\n",
        "                \n",
        "                time1 = time.perf_counter()\n",
        "                history = model.fit(X_train, y_train_cat, epochs = epochs, batch_size = batch_size, \n",
        "                                               validation_split=0.15, verbose=0)\n",
        "                time2 = time.perf_counter()\n",
        "                individual.training_time = (time2 - time1)/60\n",
        "\n",
        "                print(\"Evaluating the model with unseen data ...\")\n",
        "                time1 = time.perf_counter()\n",
        "                val_loss, val_acc = model.evaluate(X_test, y_test_cat)\n",
        "                time2 = time.perf_counter()\n",
        "                individual.process_time = ((time2 - time1)/X_test.shape[0]) * 1000\n",
        "                print(f\"Process time: {individual.process_time} milliseconds.\")\n",
        "\n",
        "                individual.accuracy = val_acc\n",
        "                individual.compute_fitness()\n",
        "\n",
        "                if val_acc > best_acc:\n",
        "                    self.population.best_individual = individual\n",
        "                    best_acc = val_acc\n",
        "\n",
        "                self.saved_cnns[individual.encoding] = individual.get_info()\n",
        "                self.saved_cnns[individual.encoding][\"epochs\"] = epochs\n",
        "\n",
        "        print(\"\\nEvery individual in the population has been evaluated! ...\")\n",
        "\n",
        "\n",
        "    def choose_winner(self, ind1, ind2):\n",
        "        if abs(ind1.accuracy, ind2.accuracy) < 0.03:\n",
        "            fastest = ind1 if ind1.process_time < ind2.process_time else ind2\n",
        "            slowest = ind1 if ind1.process_time > ind2.process_time else ind2\n",
        "\n",
        "            if fastest.process_time / slowest.process_time >= 0.7:\n",
        "                return fastest\n",
        "\n",
        "    def tournament_selection(self, num_individuals):\n",
        "\n",
        "        print(\"Tournament Selection\".center(30, \"=\"))\n",
        "\n",
        "        offspring = []\n",
        "\n",
        "        # Fill the offspring by tournament\n",
        "        while len(offspring) < num_individuals:\n",
        "            # Select two individuals\n",
        "            ind1 = random.choice(self.population.individuals)\n",
        "            ind2 = random.choice(self.population.individuals)\n",
        "\n",
        "            winner = self.choose_winner(ind1, ind2)\n",
        "\n",
        "            print(f\"Selected individuals: {ind1.encoding} acc: {round(ind1.accuracy, 4)} & {ind2.encoding} acc: {round(ind2.accuracy, 4)} Winner: {winner.encoding}\")\n",
        "\n",
        "            offspring.append(winner)\n",
        "\n",
        "        return offspring\n",
        "\n",
        "    def cross(self, parent1, parent2, cross_point):\n",
        "        # \"128_16 -mean-max-16_8\"\n",
        "        # \"max- max_64_64-64_128\"\n",
        "\n",
        "        # \"128_16-max-64_64-64_128\"\n",
        "        # \"max-mean-max-16_8\"\n",
        "        genes1 = parent1.genes[:cross_point+1] + parent2.genes[cross_point+1:]\n",
        "        genes2 = parent2.genes[:cross_point+1] + parent1.genes[cross_point+1:]\n",
        "\n",
        "        genes1 = \"-\".join(genes1)\n",
        "        genes2 = \"-\".join(genes2)\n",
        "\n",
        "        son1 = CNN(genes1, self.input_shape, self.output_shape)\n",
        "        son2 = CNN(genes2, self.input_shape, self.output_shape)\n",
        "\n",
        "        return son1, son2\n",
        "\n",
        "    def crossover(self):\n",
        "        print(\"Crossover\".center(20, \"=\"))\n",
        "        selected_indices = [] # Individuals selected for crossover\n",
        "        num_selected = 0\n",
        "\n",
        "        for index in range(self.population_size):\n",
        "            r = random.random()\n",
        "            # Se eligen los individuos de las posiciones i con a_i < prob_cruce\n",
        "            if r < self.crossover_rate:\n",
        "                selected_indices.append(index)\n",
        "                num_selected += 1\n",
        "        \n",
        "        # El número de seleccionados se hace par\n",
        "        if num_selected % 2 == 1:\n",
        "            num_selected -= 1\n",
        "\n",
        "        print(f\"Num selected: {num_selected}\")\n",
        "        print(f\"Selected indices: {selected_indices}\")\n",
        "        print(f\"Individuals size: {len(self.population.individuals)}\")\n",
        "\n",
        "        for i in range(0, num_selected, 2):\n",
        "            parent1 = self.population.individuals[selected_indices[i]]\n",
        "            parent2 = self.population.individuals[selected_indices[i+1]]\n",
        "\n",
        "            # We choose a random crossover point from shortest parent\n",
        "            shortest_parent = parent1 if parent1.num_genes < parent2.num_genes else parent2\n",
        "            cross_point = random.randint(1, shortest_parent.num_genes-1)\n",
        "\n",
        "            # We create two individuals based on their parents\n",
        "            son1, son2 = self.cross(parent1, parent2, cross_point)\n",
        "            \n",
        "            # New individuals replace their parents\n",
        "            print(f\"Parent {parent1.encoding} was replaced by {son1.encoding}\")\n",
        "            print(f\"Parent {parent2.encoding} was replaced by {son2.encoding}\")\n",
        "\n",
        "            self.population.individuals[i]   = son1\n",
        "            self.population.individuals[i+1] = son2\n",
        "\n",
        "    def mutation(self):\n",
        "\n",
        "        possible_mutations = (\"increment_depth\", \"reduce_depth\", \"change_layer_type\", \"recreate_layer\")\n",
        "\n",
        "        # We loop over the genes of every individual in population\n",
        "        for individual in self.population.individuals:\n",
        "\n",
        "            # We make a copy so we dont modify our list while iterating\n",
        "            mutated_genes = individual.genes.copy()\n",
        "\n",
        "            for n_gene, gene in enumerate(individual.genes):\n",
        "                r = random.random()\n",
        "                if r < self.mutation_rate:\n",
        "                    # Mutate\n",
        "                    print(\"Mutation\".center(20, \"=\"))\n",
        "                    print(f\"Individual to mutate: {individual.encoding}\")\n",
        "\n",
        "                    mutation_type = random.choice(possible_mutations)\n",
        "                    \n",
        "                    if mutation_type == \"increment_depth\":\n",
        "                        # Put a layer after this layer\n",
        "                        new_layer = random_layer()\n",
        "                        mutated_genes.insert(n_gene + 1, new_layer)\n",
        "                        break    \n",
        "\n",
        "                    elif mutation_type == \"reduce_depth\":\n",
        "                        # Delete the current layer\n",
        "                        removed = mutated_genes.pop(n_gene)\n",
        "                        print(f\"{removed} Removed from layers\")\n",
        "                        break\n",
        "\n",
        "                    elif mutation_type == \"change_layer_type\":\n",
        "                        # Put Skip layer if Mean layer or vice versa\n",
        "                        if gene == \"mean\" or gene == \"max\":\n",
        "                            # Generate a skip layer\n",
        "                            mutated_genes[n_gene] = skip_layer_encode()\n",
        "                        else:\n",
        "                            mutated_genes[n_gene] = pooling_layer_encode()\n",
        "                        break\n",
        "                            \n",
        "                    elif mutation_type == \"recreate_layer\":\n",
        "                        if gene == \"mean\" or gene == \"max\":\n",
        "                            # Generate a skip layer\n",
        "                            mutated_genes[n_gene] = pooling_layer_encode()\n",
        "                        else:\n",
        "                            mutated_genes[n_gene] = skip_layer_encode()\n",
        "                        break\n",
        "\n",
        "            print(f\"Individual before: {individual.genes}\")\n",
        "            individual.genes = mutated_genes\n",
        "            print(f\"Individual after: {individual.genes}\")\n",
        "\n",
        "\n",
        "    def main_loop(self):\n",
        "\n",
        "        # Initialize populatiojn\n",
        "        self.population.initialize(self.input_shape, self.output_shape)\n",
        "\n",
        "        # Print individuals\n",
        "        print(\"Starting evolution loop with the next population:\")\n",
        "        self.population.print()\n",
        "\n",
        "        # Evaluate population\n",
        "        self.evaluate_population()\n",
        "\n",
        "        print(\"Individuals Summary\".center(30, \" \"))\n",
        "        self.population.print()\n",
        "        self.save_architectures()\n",
        "\n",
        "        for generation in range(self.num_generations):\n",
        "            print(\"\".center(100, \"=\"))\n",
        "            print(f\"Generation {generation + 1}\")\n",
        "\n",
        "            # ========== Selection =============\n",
        "            self.population.individuals = self.tournament_selection(self.population_size)\n",
        "\n",
        "            # ========== Crossover ==========\n",
        "            self.crossover()\n",
        "\n",
        "            # ========== Mutation ==============\n",
        "            self.mutation()\n",
        "            \n",
        "            best_individual = self.population.best_individual\n",
        "            print(f\"The best individual was {best_individual.encoding} with accuracy {round(best_individual.accuracy, 4)}\")\n",
        "\n",
        "            print(\"Individuals Summary\".center(30, \" \"))\n",
        "            self.population.print()\n",
        "            self.save_architectures()\n",
        "\n",
        "            # ========== Evaluation ============\n",
        "            self.evaluate_population()\n",
        "\n",
        "            # =========== Elitism ==============\n",
        "            # Get worst individual only if is not already\n",
        "            best = self.population.best_individual\n",
        "            if best not in self.population.individuals:\n",
        "                self.population.individuals = sorted(self.population.individuals, key=lambda x: x.accuracy, reverse=False)\n",
        "                worst = self.population.individuals[0]\n",
        "                print(f\"Replacing individual {worst.encoding} with accuracy {worst.accuracy} with best individual {best.encoding} with accuracy {best.accuracy}\")\n",
        "                self.population.individuals[0] = best\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiUNUZd3qXhh"
      },
      "source": [
        "# Running our Genetic Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eff3OuvMqa9Q"
      },
      "source": [
        "## Preparing our parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnXXjdn-rGz4"
      },
      "source": [
        "### Lloading the architectures that have already been evaluated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "8XCGmnMyrQ9-"
      },
      "outputs": [],
      "source": [
        "# Dictionary of saved architectures with it's parameters\n",
        "# with open(\"evaluated_architectures.json\") as file:\n",
        "#     saved_cnns = json.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XqVPrIcrRfy"
      },
      "source": [
        "### Lloading the data set with which the individuals will be evaluated\n",
        "\n",
        "### Out dataset: CIFAR 10\n",
        "It has 10 classes, wich are:\n",
        "\n",
        "| Label | Description |\n",
        "|-------|-------------|\n",
        "|   0   |   airplane  |\n",
        "|   1   |  automobile |\n",
        "|   2   |     bird    |\n",
        "|   3   |     cat     |\n",
        "|   4   |     deer    |\n",
        "|   5   |     dog     |\n",
        "|   6   |     frog    |\n",
        "|   7   |    horse    |\n",
        "|   8   |     ship    |\n",
        "|   9   |    truck    |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Ia_69QZsrdQI"
      },
      "outputs": [],
      "source": [
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "y_train_cat = to_categorical(y_train)\n",
        "y_test_cat = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3XqiDf2rd0i"
      },
      "source": [
        "### Setting the training parameters ready"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "0CQ18Hbi_NcN"
      },
      "outputs": [],
      "source": [
        "training_params = {\n",
        "    \"epochs\" : 10,\n",
        "    \"batch_size\" : 32,\n",
        "    \"X_train\" : X_train,\n",
        "    \"X_test\" : X_test,\n",
        "    \"y_train_cat\" : y_train_cat,\n",
        "    \"y_test_cat\" : y_test_cat\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFFlUPBxqmiO"
      },
      "source": [
        "## Creating a GeneticAlgorithm object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "jdDTFwbb-0n6"
      },
      "outputs": [],
      "source": [
        "ga = GeneticAlgorithm(\n",
        "    population_size=5,  # How many CNN will be in the population\n",
        "    min_genes=6,        # Minimum depth of the CNN's\n",
        "    max_genes=8,       # Maximum depth of the CNN's\n",
        "    fitness_func=\"acc\", # TODO: this should be changed to a function that takes response time into account\n",
        "    num_generations=6,\n",
        "    mutation_rate=0.05, # Mutation rate (value by convention)\n",
        "    crossover_rate=0.4, # Crossover rate (value by convention)\n",
        "    saved_cnns={}, # Document that saves the individuals who have already been evaluated, thus saving resources\n",
        "    training_params=training_params # Parameters needed for training\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bd7XysHZDQKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35dda286-48e1-49e0-83e8-5e37061ede4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting evolution loop with the next population:\n",
            " CNN Depth   Encoding                                               Accuracy    Process time    Fitness\n",
            "-----------  ----------------------------------------------------  ----------  --------------  ---------\n",
            "     8       256_256-32_256-max-mean-512_256-32_512-64_128-mean        0             0             0\n",
            "     6       max-32_512-256_512-256_128-mean-mean                      0             0             0\n",
            "     8       mean-128_64-128_64-128_256-max-256_256-128_256-32_64      0             0             0\n",
            "     7       mean-mean-max-512_32-mean-max-256_512                     0             0             0\n",
            "     6       32_64-128_128-mean-256_32-32_32-512_128                   0             0             0\n",
            "     6       32_32-128_32-64_32-max-64_128-128_512                     0             0             0\n",
            "     8       mean-mean-256_128-mean-64_32-128_64-128_128-256_256       0             0             0\n",
            "     8       32_32-max-max-mean-32_256-32_32-mean-max                  0             0             0\n",
            "     6       max-mean-max-max-mean-128_512                             0             0             0\n",
            "     7       64_256-256_128-64_32-128_256-256_256-mean-128_512         0             0             0\n",
            "====================================================================================================\n",
            "Architecture: 256_256-32_256-max-mean-512_256-32_512-64_128-mean, Depth: 8\n",
            "Training ...\n",
            "Evaluating the model with unseen data ...\n",
            "313/313 [==============================] - 5s 16ms/step - loss: 0.6784 - accuracy: 0.8112\n",
            "Process time: 0.5174107188000562 milliseconds.\n",
            "====================================================================================================\n",
            "Architecture: max-32_512-256_512-256_128-mean-mean, Depth: 6\n",
            "Training ...\n",
            "Evaluating the model with unseen data ...\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.9567 - accuracy: 0.7017\n",
            "Process time: 0.5230839796999135 milliseconds.\n",
            "====================================================================================================\n",
            "Architecture: mean-128_64-128_64-128_256-max-256_256-128_256-32_64, Depth: 8\n",
            "Training ...\n",
            "Evaluating the model with unseen data ...\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.9117 - accuracy: 0.7435\n",
            "Process time: 0.21855637530006788 milliseconds.\n",
            "====================================================================================================\n",
            "Architecture: mean-mean-max-512_32-mean-max-256_512, Depth: 7\n",
            "Training ...\n",
            "Evaluating the model with unseen data ...\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 1.5486 - accuracy: 0.4453\n",
            "Process time: 0.14290096330005325 milliseconds.\n",
            "====================================================================================================\n",
            "Architecture: 32_64-128_128-mean-256_32-32_32-512_128, Depth: 6\n",
            "Training ...\n",
            "Evaluating the model with unseen data ...\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.9131 - accuracy: 0.7157\n",
            "Process time: 0.523226353300015 milliseconds.\n",
            "====================================================================================================\n",
            "Architecture: 32_32-128_32-64_32-max-64_128-128_512, Depth: 6\n",
            "Training ...\n",
            "Evaluating the model with unseen data ...\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.7053 - accuracy: 0.7949\n",
            "Process time: 0.28441813410008765 milliseconds.\n",
            "====================================================================================================\n",
            "Architecture: mean-mean-256_128-mean-64_32-128_64-128_128-256_256, Depth: 8\n",
            "Training ...\n",
            "Evaluating the model with unseen data ...\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 1.4691 - accuracy: 0.5767\n",
            "Process time: 0.16308691890008048 milliseconds.\n",
            "====================================================================================================\n",
            "Architecture: 32_32-max-max-mean-32_256-32_32-mean-max, Depth: 8\n",
            "Training ...\n",
            "Evaluating the model with unseen data ...\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.8683 - accuracy: 0.7014\n",
            "Process time: 0.12483664700012014 milliseconds.\n",
            "====================================================================================================\n",
            "Architecture: max-mean-max-max-mean-128_512, Depth: 6\n",
            "Training ...\n",
            "Evaluating the model with unseen data ...\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 2.0870 - accuracy: 0.2152\n",
            "Process time: 0.11674783369999205 milliseconds.\n",
            "====================================================================================================\n",
            "Architecture: 64_256-256_128-64_32-128_256-256_256-mean-128_512, Depth: 7\n",
            "Training ...\n",
            "Evaluating the model with unseen data ...\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 0.6225 - accuracy: 0.8327\n",
            "Process time: 1.0364571473999604 milliseconds.\n",
            "\n",
            "Every individual in the population has been evaluated! ...\n",
            "     Individuals Summary      \n",
            " CNN Depth   Encoding                                               Accuracy    Process time    Fitness\n",
            "-----------  ----------------------------------------------------  ----------  --------------  ---------\n",
            "     8       256_256-32_256-max-mean-512_256-32_512-64_128-mean      0.8112        0.5174       1.5678\n",
            "     6       max-32_512-256_512-256_128-mean-mean                    0.7017        0.5231       1.3415\n",
            "     8       mean-128_64-128_64-128_256-max-256_256-128_256-32_64    0.7435        0.2186       3.4019\n",
            "     7       mean-mean-max-512_32-mean-max-256_512                   0.4453        0.1429       3.1161\n",
            "     6       32_64-128_128-mean-256_32-32_32-512_128                 0.7157        0.5232       1.3679\n",
            "     6       32_32-128_32-64_32-max-64_128-128_512                   0.7949        0.2844       2.7948\n",
            "     8       mean-mean-256_128-mean-64_32-128_64-128_128-256_256     0.5767        0.1631       3.5362\n",
            "     8       32_32-max-max-mean-32_256-32_32-mean-max                0.7014        0.1248       5.6185\n",
            "     6       max-mean-max-max-mean-128_512                           0.2152        0.1167       1.8433\n",
            "     7       64_256-256_128-64_32-128_256-256_256-mean-128_512       0.8327        1.0365       0.8034\n",
            "====================================================================================================\n",
            "Generation 1\n",
            "=====Tournament Selection=====\n",
            "Selected individuals: mean-mean-256_128-mean-64_32-128_64-128_128-256_256 acc: 0.5767 & 64_256-256_128-64_32-128_256-256_256-mean-128_512 acc: 0.8327 Winner: 64_256-256_128-64_32-128_256-256_256-mean-128_512\n",
            "Selected individuals: mean-mean-max-512_32-mean-max-256_512 acc: 0.4453 & 32_32-128_32-64_32-max-64_128-128_512 acc: 0.7949 Winner: 32_32-128_32-64_32-max-64_128-128_512\n",
            "Selected individuals: 256_256-32_256-max-mean-512_256-32_512-64_128-mean acc: 0.8112 & max-mean-max-max-mean-128_512 acc: 0.2152 Winner: 256_256-32_256-max-mean-512_256-32_512-64_128-mean\n",
            "Selected individuals: max-mean-max-max-mean-128_512 acc: 0.2152 & 256_256-32_256-max-mean-512_256-32_512-64_128-mean acc: 0.8112 Winner: 256_256-32_256-max-mean-512_256-32_512-64_128-mean\n",
            "Selected individuals: 256_256-32_256-max-mean-512_256-32_512-64_128-mean acc: 0.8112 & 32_64-128_128-mean-256_32-32_32-512_128 acc: 0.7157 Winner: 256_256-32_256-max-mean-512_256-32_512-64_128-mean\n",
            "Selected individuals: mean-mean-256_128-mean-64_32-128_64-128_128-256_256 acc: 0.5767 & mean-mean-256_128-mean-64_32-128_64-128_128-256_256 acc: 0.5767 Winner: mean-mean-256_128-mean-64_32-128_64-128_128-256_256\n",
            "Selected individuals: max-32_512-256_512-256_128-mean-mean acc: 0.7017 & mean-128_64-128_64-128_256-max-256_256-128_256-32_64 acc: 0.7435 Winner: mean-128_64-128_64-128_256-max-256_256-128_256-32_64\n",
            "Selected individuals: 32_32-max-max-mean-32_256-32_32-mean-max acc: 0.7014 & mean-mean-256_128-mean-64_32-128_64-128_128-256_256 acc: 0.5767 Winner: 32_32-max-max-mean-32_256-32_32-mean-max\n",
            "Selected individuals: mean-mean-256_128-mean-64_32-128_64-128_128-256_256 acc: 0.5767 & max-mean-max-max-mean-128_512 acc: 0.2152 Winner: mean-mean-256_128-mean-64_32-128_64-128_128-256_256\n",
            "Selected individuals: mean-mean-256_128-mean-64_32-128_64-128_128-256_256 acc: 0.5767 & 32_64-128_128-mean-256_32-32_32-512_128 acc: 0.7157 Winner: 32_64-128_128-mean-256_32-32_32-512_128\n",
            "=====Crossover======\n",
            "Num selected: 6\n",
            "Selected indices: [0, 1, 2, 3, 4, 8, 9]\n",
            "Individuals size: 10\n",
            "Parent 64_256-256_128-64_32-128_256-256_256-mean-128_512 was replaced by 64_256-256_128-64_32-128_256-256_256-mean\n",
            "Parent 32_32-128_32-64_32-max-64_128-128_512 was replaced by 32_32-128_32-64_32-max-64_128-128_512-128_512\n",
            "Parent 256_256-32_256-max-mean-512_256-32_512-64_128-mean was replaced by 256_256-32_256-max-mean-512_256-32_512-64_128-mean\n",
            "Parent 256_256-32_256-max-mean-512_256-32_512-64_128-mean was replaced by 256_256-32_256-max-mean-512_256-32_512-64_128-mean\n",
            "Parent 256_256-32_256-max-mean-512_256-32_512-64_128-mean was replaced by 256_256-32_256-256_128-mean-64_32-128_64-128_128-256_256\n",
            "Parent mean-mean-256_128-mean-64_32-128_64-128_128-256_256 was replaced by mean-mean-max-mean-512_256-32_512-64_128-mean\n",
            "======Mutation======\n",
            "Individual to mutate: 64_256-256_128-64_32-128_256-256_256-mean\n",
            "Individual before: ['64_256', '256_128', '64_32', '128_256', '256_256', 'mean']\n",
            "Individual after: ['64_256', '32_32', '256_128', '64_32', '128_256', '256_256', 'mean']\n",
            "Individual before: ['32_32', '128_32', '64_32', 'max', '64_128', '128_512', '128_512']\n",
            "Individual after: ['32_32', '128_32', '64_32', 'max', '64_128', '128_512', '128_512']\n",
            "Individual before: ['256_256', '32_256', 'max', 'mean', '512_256', '32_512', '64_128', 'mean']\n",
            "Individual after: ['256_256', '32_256', 'max', 'mean', '512_256', '32_512', '64_128', 'mean']\n",
            "======Mutation======\n",
            "Individual to mutate: 256_256-32_256-max-mean-512_256-32_512-64_128-mean\n",
            "64_128 Removed from layers\n",
            "Individual before: ['256_256', '32_256', 'max', 'mean', '512_256', '32_512', '64_128', 'mean']\n",
            "Individual after: ['256_256', '32_256', 'max', 'mean', '512_256', '32_512', 'mean']\n",
            "Individual before: ['256_256', '32_256', '256_128', 'mean', '64_32', '128_64', '128_128', '256_256']\n",
            "Individual after: ['256_256', '32_256', '256_128', 'mean', '64_32', '128_64', '128_128', '256_256']\n",
            "======Mutation======\n",
            "Individual to mutate: mean-mean-max-mean-512_256-32_512-64_128-mean\n",
            "Individual before: ['mean', 'mean', 'max', 'mean', '512_256', '32_512', '64_128', 'mean']\n",
            "Individual after: ['mean', 'mean', 'max', '64_256', '512_256', '32_512', '64_128', 'mean']\n",
            "======Mutation======\n",
            "Individual to mutate: mean-128_64-128_64-128_256-max-256_256-128_256-32_64\n",
            "Individual before: ['mean', '128_64', '128_64', '128_256', 'max', '256_256', '128_256', '32_64']\n",
            "Individual after: ['mean', '128_64', '128_64', '128_256', 'max', '256_64', '128_256', '32_64']\n",
            "Individual before: ['32_32', 'max', 'max', 'mean', '32_256', '32_32', 'mean', 'max']\n",
            "Individual after: ['32_32', 'max', 'max', 'mean', '32_256', '32_32', 'mean', 'max']\n",
            "======Mutation======\n",
            "Individual to mutate: mean-mean-256_128-mean-64_32-128_64-128_128-256_256\n",
            "64_32 Removed from layers\n",
            "Individual before: ['mean', 'mean', '256_128', 'mean', '64_32', '128_64', '128_128', '256_256']\n",
            "Individual after: ['mean', 'mean', '256_128', 'mean', '128_64', '128_128', '256_256']\n",
            "======Mutation======\n",
            "Individual to mutate: 32_64-128_128-mean-256_32-32_32-512_128\n",
            "32_32 Removed from layers\n",
            "Individual before: ['32_64', '128_128', 'mean', '256_32', '32_32', '512_128']\n",
            "Individual after: ['32_64', '128_128', 'mean', '256_32', '512_128']\n",
            "The best individual was 64_256-256_128-64_32-128_256-256_256-mean-128_512 with accuracy 0.8327\n",
            "     Individuals Summary      \n",
            " CNN Depth   Encoding                                                   Accuracy    Process time    Fitness\n",
            "-----------  --------------------------------------------------------  ----------  --------------  ---------\n",
            "     6       64_256-256_128-64_32-128_256-256_256-mean                     0             0             0\n",
            "     7       32_32-128_32-64_32-max-64_128-128_512-128_512                 0             0             0\n",
            "     8       256_256-32_256-max-mean-512_256-32_512-64_128-mean            0             0             0\n",
            "     8       256_256-32_256-max-mean-512_256-32_512-64_128-mean            0             0             0\n",
            "     8       256_256-32_256-256_128-mean-64_32-128_64-128_128-256_256      0             0             0\n",
            "     8       mean-mean-max-mean-512_256-32_512-64_128-mean                 0             0             0\n",
            "     8       mean-128_64-128_64-128_256-max-256_256-128_256-32_64        0.7435        0.2186       3.4019\n",
            "     8       32_32-max-max-mean-32_256-32_32-mean-max                    0.7014        0.1248       5.6185\n",
            "     8       mean-mean-256_128-mean-64_32-128_64-128_128-256_256         0.5767        0.1631       3.5362\n",
            "     6       32_64-128_128-mean-256_32-32_32-512_128                     0.7157        0.5232       1.3679\n",
            "====================================================================================================\n",
            "Architecture: 64_256-256_128-64_32-128_256-256_256-mean, Depth: 6\n",
            "Training ...\n"
          ]
        }
      ],
      "source": [
        "t1 = time.perf_counter()\n",
        "ga.main_loop()\n",
        "t2 = time.perf_counter()\n",
        "print(f\"Time taken: {t2-t1} seconds.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIRVe3QJ4UIH"
      },
      "outputs": [],
      "source": [
        "# ga.saved_cnns = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOn4sN4t4UIH"
      },
      "outputs": [],
      "source": [
        "# from concurrent.futures import ProcessPoolExecutor\n",
        "# t1 = time.perf_counter()\n",
        "# with ProcessPoolExecutor() as executor:\n",
        "#     executor.map(ga.main_loop())\n",
        "# t2 = time.perf_counter()\n",
        "# print(f\"Time taken: {t2-t1} seconds.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoA6yPbO4UIH"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "multiprocessing.cpu_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1CBmS3F4UIH"
      },
      "outputs": [],
      "source": [
        "ga.population.best_individual"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "buWhdfrPct0L"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "AutoCNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}